{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q2-3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6MlHCEAdZPy1","colab_type":"text"},"source":["**Initial step:** Please try to put the extracted CamVid folder in your Google Drive!\n","So now you could mount your data to this ipynb!"]},{"cell_type":"code","metadata":{"id":"UGAXY5HMYjzL","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wTBYCMcpZoA8","colab_type":"code","colab":{}},"source":["# if you mount Google drive correctly, the following commands should be able to executed correctly\n","!ls /content/drive/\n","%cd \"/content/drive/My Drive/Colab Notebooks/HW4_updated1/\"\n","%cd \"CamVid\"\n","\n","!ls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pfWmPZKyZ8gA","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import utils\n","import torchvision\n","from torchvision import models\n","from torchvision.models.vgg import VGG\n","import random\n","\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import time\n","import sys\n","import os\n","from os import path\n","\n","from PIL import Image\n","import pandas as pd\n","from torchvision.models.vgg import VGG\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bxRR6L-k3Hjq","colab_type":"code","colab":{}},"source":["seed = 1450\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YMzL2KhcaI9u","colab_type":"code","outputId":"bd79e8ae-fc40-4c33-fc36-ace3f7ce3469","executionInfo":{"status":"ok","timestamp":1576770699811,"user_tz":-480,"elapsed":54789,"user":{"displayName":"張聿程","photoUrl":"","userId":"08620233356790329924"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["\n","root_dir   = \"/content/drive/My Drive/Colab Notebooks/HW4_updated1/CamVid/\"\n","train_file = os.path.join(root_dir, \"train.csv\")\n","val_file   = os.path.join(root_dir, \"val.csv\")\n","\n","print(\"training csv exits:{}\".format(path.exists(train_file)))\n","print(\"validation csv exits:{}\".format(path.exists(val_file)))\n","\n","# the folder to save results for comparison\n","folder_to_save_validation_result = '/content/drive/My Drive/Colab Notebooks/HW4_updated1/CamVid/result_comparision/' \n","\n","if os.path.isdir(folder_to_save_validation_result) == False:\n","    os.mkdir(folder_to_save_validation_result)\n","\n","\n","# the number of segmentation classes\n","num_class = 3 # 32 for original CamVid\n","means     = np.array([103.939, 116.779, 123.68]) / 255. # mean of three channels in the order of BGR\n","\n","h, w      = 256, 256\n","train_h = 256\n","train_w = 256\n","val_h = 256\n","val_w = 256\n","\n","## parameters for Solver-Adam in this example\n","batch_size = 6 #\n","epochs     = 20 # don't try to improve the performance by simply increasing the training epochs or iterations\n","lr         = 1e-4    # achieved besty results \n","step_size  = 100 # Won't work when epochs <=100\n","gamma      = 0.5 # \n","#\n","\n","## index for validation images\n","global_index = 0\n","\n","# pixel accuracy and mIOU list \n","pixel_acc_list = []\n","mIOU_list = []\n","\n","use_gpu = torch.cuda.is_available()\n","num_gpu = list(range(torch.cuda.device_count()))\n","\n","class CamVidDataset(Dataset):\n","\n","    def __init__(self, csv_file, phase, n_class=num_class, crop=True, flip_rate=0.5):\n","        self.data      = pd.read_csv(csv_file)\n","        self.means     = means\n","        self.n_class   = n_class\n","        self.flip_rate = flip_rate       \n","\n","        self.resize_h = h\n","        self.resize_w = w        \n","        \n","        if phase == 'train':\n","            self.new_h = train_h\n","            self.new_w = train_w\n","            self.crop = crop\n","        elif phase == 'val':\n","            self.flip_rate = 0.\n","            self.crop = False # False\n","            self.new_h = val_h\n","            self.new_w = val_w\n","\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_name   = self.data.iloc[idx, 0]                \n","        img_name = root_dir  + img_name                        \n","        img = Image.open(img_name).convert('RGB')  \n","\n","        label_name = self.data.iloc[idx, 1]        \n","        label_name = root_dir  + label_name                       \n","        label_image = Image.open(label_name)\n","        label = np.asarray(label_image)\n","\n","        # In training mode, the crop strategy is random-shift crop.\n","        # In validation model, it is center crop.\n","        if self.crop:            \n","            w, h = img.size\n","            A_x_offset = np.int32(np.random.randint(0, w - self.new_w + 1, 1))[0]\n","            A_y_offset = np.int32(np.random.randint(0, h - self.new_h + 1, 1))[0]\n","\n","            img = img.crop((A_x_offset, A_y_offset, A_x_offset + self.new_w, A_y_offset + self.new_h)) # left, top, right, bottom\n","            label_image = label_image.crop((A_x_offset, A_y_offset, A_x_offset + self.new_w, A_y_offset + self.new_h)) # left, top, right, bottom\n","        else:            \n","            w, h = img.size\n","            A_x_offset = int((w - self.new_w)/2)\n","            A_y_offset = int((h - self.new_h)/2)\n","            \n","            img = img.crop((A_x_offset, A_y_offset, A_x_offset + self.new_w, A_y_offset + self.new_h)) # left, top, right, bottom\n","            label_image = label_image.crop((A_x_offset, A_y_offset, A_x_offset + self.new_w, A_y_offset + self.new_h)) # left, top, right, bottom\n","\n","            label_image_h, label_image_w = label_image.size\n","\n","        # we could try to revise the values in label for reducing the number of segmentation classes\n","        label = np.array(label_image)\n","        label[label == 2] = 1\n","        label[label == 3] = 1\n","        label[label == 4] = 1\n","        label[label == 5] = 1\n","        label[label == 6] = 1\n","        label[label == 7] = 1\n","        label[label == 11] = 1\n","        label[label == 8] = 2\n","        label[label == 9] = 2\n","        label[label == 10] = 2\n","\n","        if random.random() < self.flip_rate:\n","            img   = np.fliplr(img)\n","            label = np.fliplr(label)\n","        \n","        # reduce mean in terms of BGR\n","        img = np.transpose(img, (2, 0, 1)) / 255.\n","        img[0] -= self.means[0]\n","        img[1] -= self.means[1]\n","        img[2] -= self.means[2]\n","\n","        # convert to tensor\n","        img = torch.from_numpy(img.copy()).float()\n","        label = torch.from_numpy(label.copy()).long()\n","\n","        # create one-hot encoding\n","        h, w = label.size()\n","        target = torch.zeros(self.n_class, h, w)\n","        for c in range(self.n_class):\n","            target[c][label == c] = 1\n","\n","        sample = {'X': img, 'Y': target, 'l': label}\n","\n","        return sample\n","\n","\n","train_data = CamVidDataset(csv_file=train_file, phase='train')\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=8)\n","\n","val_data = CamVidDataset(csv_file=val_file, phase='val', flip_rate=0)\n","val_loader = DataLoader(val_data, batch_size=1, num_workers=8)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["training csv exits:True\n","validation csv exits:True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4h-a5moWhLhF","colab_type":"code","colab":{}},"source":["# cropped version from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n","cfg = {\n","    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","ranges = {\n","    'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n","    'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n","    'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n","    'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n","}\n","\n","def make_layers(cfg, batch_norm=False):\n","    layers = []\n","    in_channels = 3\n","    for v in cfg:\n","        if v == 'M':\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","            if batch_norm:\n","                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n","            else:\n","                layers += [conv2d, nn.ReLU(inplace=True)]\n","            in_channels = v\n","    return nn.Sequential(*layers)\n","\n","class VGGNet(VGG):\n","    def __init__(self, pretrained=True, model='vgg16', requires_grad=True, remove_fc=True, show_params=False):\n","        super().__init__(make_layers(cfg[model]))\n","        self.ranges = ranges[model]\n","\n","        if pretrained:            \n","            exec(\"self.load_state_dict(models.%s(pretrained=True).state_dict())\" % model)\n","\n","        if not requires_grad:\n","            for param in super().parameters():\n","                param.requires_grad = False\n","\n","        if remove_fc:  # delete redundant fully-connected layer params, can save memory\n","            del self.classifier\n","\n","        if show_params:\n","            for name, param in self.named_parameters():\n","                print(name, param.size())\n","\n","    def forward(self, x):\n","        output = {}\n","\n","        # get the output of each maxpooling layer (5 maxpool in VGG net)\n","        for idx in range(len(self.ranges)):\n","            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):\n","                x = self.features[layer](x)\n","            output[\"x%d\"%(idx+1)] = x\n","\n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZWlA75uj4u1","colab_type":"code","colab":{}},"source":["class FCN8s(nn.Module):\n","    #Ref: https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1\n","    #The layer description is accordance with the above fiture instead of the original paper. Alex 2019/12/03 \n","    def __init__(self, pretrained_net, n_class):\n","        super().__init__()\n","        self.n_class = n_class\n","        self.pretrained_net = pretrained_net\n","        self.relu    = nn.ReLU(inplace=True)\n","        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn1     = nn.BatchNorm2d(512)\n","        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn2     = nn.BatchNorm2d(256)\n","        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn3     = nn.BatchNorm2d(128)\n","        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn4     = nn.BatchNorm2d(64)\n","        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn5     = nn.BatchNorm2d(32)\n","        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n","\n","    def forward(self, x):\n","        output = self.pretrained_net(x)\n","        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n","        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n","        x3 = output['x3']  # size=(N, 256, x.H/8,  x.W/8)\n","\n","        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n","        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)                      \n","        score = self.relu(self.deconv2(score))            # size=(N, 256, x.H/8, x.W/8)\n","        score = self.bn2(score + x3)                      # element-wise add, size=(N, 256, x.H/8, x.W/8)           \n","        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n","        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n","        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n","        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n","\n","        return score  # size=(N, n_class, x.H/1, x.W/1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_d5x1O2j53K","colab_type":"code","outputId":"30fa92cc-851e-4df2-f570-cf6ce7109849","executionInfo":{"status":"ok","timestamp":1576770751901,"user_tz":-480,"elapsed":106853,"user":{"displayName":"張聿程","photoUrl":"","userId":"08620233356790329924"}},"colab":{"base_uri":"https://localhost:8080/","height":972}},"source":["# load pretrained weights and define FCN8s\n","vgg_model = VGGNet(requires_grad=True, remove_fc=True)\n","fcn_model = FCN8s(pretrained_net=vgg_model, n_class=num_class)\n","\n","ts = time.time()\n","vgg_model = vgg_model.cuda()\n","fcn_model = fcn_model.cuda()\n","fcn_model = nn.DataParallel(fcn_model, device_ids=num_gpu)\n","print(\"Finish cuda loading, time elapsed {}\".format(time.time() - ts))\n","\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(fcn_model.parameters(), lr=lr)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n","\n","print(fcn_model)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:42<00:00, 13.1MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Finish cuda loading, time elapsed 5.294675350189209\n","DataParallel(\n","  (module): FCN8s(\n","    (pretrained_net): VGGNet(\n","      (features): Sequential(\n","        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (3): ReLU(inplace=True)\n","        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (6): ReLU(inplace=True)\n","        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (8): ReLU(inplace=True)\n","        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (11): ReLU(inplace=True)\n","        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (13): ReLU(inplace=True)\n","        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (15): ReLU(inplace=True)\n","        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (18): ReLU(inplace=True)\n","        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (20): ReLU(inplace=True)\n","        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (22): ReLU(inplace=True)\n","        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (25): ReLU(inplace=True)\n","        (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (27): ReLU(inplace=True)\n","        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (29): ReLU(inplace=True)\n","        (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      )\n","      (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","    )\n","    (relu): ReLU(inplace=True)\n","    (deconv1): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (deconv2): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (deconv3): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (deconv4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (deconv5): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (classifier): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"scngbKimJU0P","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oExd6u7kkLw6","colab_type":"code","colab":{}},"source":["def val(epoch):\n","    fcn_model.eval()\n","    total_ious = []\n","    pixel_accs = []\n","                    \n","    \n","    for iter, batch in enumerate(val_loader): ## batch is 1 in this case\n","        if use_gpu:\n","            inputs = Variable(batch['X'].cuda())\n","        else:\n","            inputs = Variable(batch['X'])        \n","\n","        output = fcn_model(inputs)                                \n","        \n","        # only save the 1st image for comparison\n","        if iter == 0:\n","            print('---------iter={}'.format(iter))\n","            # generate images\n","            images = output.data.max(1)[1].cpu().numpy()[:,:,:]\n","            image = images[0,:,:]        \n","            save_result_comparison(batch['X'], image)\n","                            \n","        output = output.data.cpu().numpy()\n","\n","        N, _, h, w = output.shape                \n","        pred = output.transpose(0, 2, 3, 1).reshape(-1, num_class).argmax(axis=1).reshape(N, h, w)        \n","        target = batch['l'].cpu().numpy().reshape(N, h, w)\n","\n","        for p, t in zip(pred, target):\n","            total_ious.append(iou(p, t))\n","            pixel_accs.append(pixel_acc(p, t))\n","\n","    # Calculate average IoU\n","    total_ious = np.array(total_ious).T  # n_class * val_len\n","    ious = np.nanmean(total_ious, axis=1)\n","    pixel_accs = np.array(pixel_accs).mean()\n","    print(\"epoch{}, pix_acc: {}, meanIoU: {}, IoUs: {}\".format(epoch, pixel_accs, np.nanmean(ious), ious))\n","    \n","    global pixel_acc_list\n","    global mIOU_list\n","    \n","    pixel_acc_list.append(pixel_accs)\n","    mIOU_list.append(np.nanmean(ious))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lUjPeffKbm36","colab_type":"code","colab":{}},"source":["def train():\n","    for epoch in range(epochs):\n","        scheduler.step()\n","\n","        ts = time.time()\n","        for iter, batch in enumerate(train_loader):\n","            optimizer.zero_grad()\n","\n","            if use_gpu:\n","                inputs = Variable(batch['X'].cuda())\n","                labels = Variable(batch['Y'].cuda())\n","            else:\n","                inputs, labels = Variable(batch['X']), Variable(batch['Y'])\n","\n","            outputs = fcn_model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            if iter % 10 == 0:\n","                print(\"epoch{}, iter{}, loss: {}\".format(epoch, iter, loss.data.item()))\n","        \n","        print(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n","        \n","\n","        val(epoch)\n","        \n","    highest_pixel_acc = max(pixel_acc_list)\n","    highest_mIOU = max(mIOU_list)        \n","    \n","    highest_pixel_acc_epoch = pixel_acc_list.index(highest_pixel_acc)\n","    highest_mIOU_epoch = mIOU_list.index(highest_mIOU)\n","    \n","    print(\"The highest mIOU is {} and is achieved at epoch-{}\".format(highest_mIOU, highest_mIOU_epoch))\n","    print(\"The highest pixel accuracy  is {} and is achieved at epoch-{}\".format(highest_pixel_acc, highest_pixel_acc_epoch))  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qj9aXgiYkYfW","colab_type":"code","colab":{}},"source":["def save_result_comparison(input_np, output_np):\n","    means     = np.array([103.939, 116.779, 123.68]) / 255.\n","    \n","    global global_index\n","    \n","    original_im_RGB = np.zeros((256,256,3))    \n","    original_im_RGB[:,:,0] = input_np[0,0,:,:]    \n","    original_im_RGB[:,:,1] = input_np[0,1,:,:]\n","    original_im_RGB[:,:,2] = input_np[0,2,:,:]\n","        \n","    original_im_RGB[:,:,0] = original_im_RGB[:,:,0] + means[0]\n","    original_im_RGB[:,:,1] = original_im_RGB[:,:,1] + means[1]\n","    original_im_RGB[:,:,2] = original_im_RGB[:,:,2] + means[2]\n","        \n","    original_im_RGB[:,:,0] = original_im_RGB[:,:,0]*255.0\n","    original_im_RGB[:,:,1] = original_im_RGB[:,:,1]*255.0\n","    original_im_RGB[:,:,2] = original_im_RGB[:,:,2]*255.0\n","    \n","    im_seg_RGB = np.zeros((256,256,3))\n","\n","    # the following version is designed for 11-class version and could still work if the number of classes is fewer.\n","    for i in range(256):\n","        for j in range(256):\n","            if output_np[i,j] == 0:\n","                im_seg_RGB[i,j,:] = [128, 128, 128]\n","            elif output_np[i,j] == 1:  \n","                im_seg_RGB[i,j,:] = [0, 0, 0]\n","            elif output_np[i,j] == 2:  \n","                im_seg_RGB[i,j,:] = [0, 128, 192]      \n","                    \n","    # horizontally stack original image and its corresponding segmentation results     \n","    hstack_image = np.hstack((original_im_RGB, im_seg_RGB))             \n","    new_im = Image.fromarray(np.uint8(hstack_image))\n","    \n","    file_name = folder_to_save_validation_result + str(global_index) + '.jpg'\n","        \n","    global_index = global_index + 1\n","        \n","    new_im.save(file_name)     "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E7cK6h9vkbf7","colab_type":"code","colab":{}},"source":["# borrow functions and modify it from https://github.com/Kaixhin/FCN-semantic-segmentation/blob/master/main.py\n","# Calculates class intersections over unions\n","def iou(pred, target):\n","    ious = []\n","    for cls in range(num_class):\n","        pred_inds = pred == cls\n","        target_inds = target == cls\n","        intersection = pred_inds[target_inds].sum()\n","        union = pred_inds.sum() + target_inds.sum() - intersection\n","        if union == 0:\n","            ious.append(float('nan'))  # if there is no ground truth, do not include in evaluation\n","        else:\n","            ious.append(float(intersection) / max(union, 1))\n","        # print(\"cls\", cls, pred_inds.sum(), target_inds.sum(), intersection, float(intersection) / max(union, 1))\n","    return ious\n","\n","\n","def pixel_acc(pred, target):\n","    correct = (pred == target).sum()\n","    total   = (target == target).sum()\n","    return correct / total"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KrYQIIobkPZk","colab_type":"code","outputId":"f8cf9547-0c16-4095-ff2b-882ccfc10d2f","executionInfo":{"status":"ok","timestamp":1576771098422,"user_tz":-480,"elapsed":453343,"user":{"displayName":"張聿程","photoUrl":"","userId":"08620233356790329924"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["## perform training and validation\n","val(0)  # show the accuracy before training\n","train()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["---------iter=0\n","epoch0, pix_acc: 0.8266966247558594, meanIoU: 0.27556554158528646, IoUs: [0.         0.82669662 0.        ]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["epoch0, iter0, loss: 0.6383957266807556\n","epoch0, iter10, loss: 0.30079206824302673\n","epoch0, iter20, loss: 0.4100785255432129\n","epoch0, iter30, loss: 0.34884023666381836\n","epoch0, iter40, loss: 0.3070909082889557\n","epoch0, iter50, loss: 0.21330513060092926\n","epoch0, iter60, loss: 0.2893697917461395\n","Finish epoch 0, time elapsed 48.0042941570282\n","---------iter=0\n","epoch0, pix_acc: 0.8266966247558594, meanIoU: 0.27556554158528646, IoUs: [0.         0.82669662 0.        ]\n","epoch1, iter0, loss: 0.2504749000072479\n","epoch1, iter10, loss: 0.33045142889022827\n","epoch1, iter20, loss: 0.21983668208122253\n","epoch1, iter30, loss: 0.25352737307548523\n","epoch1, iter40, loss: 0.16532069444656372\n","epoch1, iter50, loss: 0.15407320857048035\n","epoch1, iter60, loss: 0.1486133188009262\n","Finish epoch 1, time elapsed 11.061439275741577\n","---------iter=0\n","epoch1, pix_acc: 0.92413330078125, meanIoU: 0.6013517554136001, IoUs: [0.88904911 0.91500615 0.        ]\n","epoch2, iter0, loss: 0.13225553929805756\n","epoch2, iter10, loss: 0.14432427287101746\n","epoch2, iter20, loss: 0.09766874462366104\n","epoch2, iter30, loss: 0.13040879368782043\n","epoch2, iter40, loss: 0.12706661224365234\n","epoch2, iter50, loss: 0.09480159729719162\n","epoch2, iter60, loss: 0.11462491005659103\n","Finish epoch 2, time elapsed 11.065248012542725\n","---------iter=0\n","epoch2, pix_acc: 0.945428466796875, meanIoU: 0.7720431374627025, IoUs: [0.89666857 0.93534404 0.4841168 ]\n","epoch3, iter0, loss: 0.11373291909694672\n","epoch3, iter10, loss: 0.1395406723022461\n","epoch3, iter20, loss: 0.087211474776268\n","epoch3, iter30, loss: 0.11354247480630875\n","epoch3, iter40, loss: 0.12602607905864716\n","epoch3, iter50, loss: 0.07435214519500732\n","epoch3, iter60, loss: 0.09605244547128677\n","Finish epoch 3, time elapsed 11.244872808456421\n","---------iter=0\n","epoch3, pix_acc: 0.9536091613769532, meanIoU: 0.7951602756347219, IoUs: [0.90234408 0.94470278 0.53843396]\n","epoch4, iter0, loss: 0.08566597104072571\n","epoch4, iter10, loss: 0.15105481445789337\n","epoch4, iter20, loss: 0.09620580077171326\n","epoch4, iter30, loss: 0.07464536279439926\n","epoch4, iter40, loss: 0.04930936172604561\n","epoch4, iter50, loss: 0.07010003179311752\n","epoch4, iter60, loss: 0.08650662750005722\n","Finish epoch 4, time elapsed 11.094434261322021\n","---------iter=0\n","epoch4, pix_acc: 0.9615205383300781, meanIoU: 0.8267631688319463, IoUs: [0.91082751 0.95421036 0.61525164]\n","epoch5, iter0, loss: 0.10607174038887024\n","epoch5, iter10, loss: 0.08732030540704727\n","epoch5, iter20, loss: 0.08718287944793701\n","epoch5, iter30, loss: 0.0713038444519043\n","epoch5, iter40, loss: 0.06514862924814224\n","epoch5, iter50, loss: 0.07334413379430771\n","epoch5, iter60, loss: 0.06503070890903473\n","Finish epoch 5, time elapsed 11.181568622589111\n","---------iter=0\n","epoch5, pix_acc: 0.9559654235839844, meanIoU: 0.7939398861845399, IoUs: [0.91336194 0.94779181 0.52066591]\n","epoch6, iter0, loss: 0.0961795225739479\n","epoch6, iter10, loss: 0.06150274723768234\n","epoch6, iter20, loss: 0.05556191876530647\n","epoch6, iter30, loss: 0.06265734136104584\n","epoch6, iter40, loss: 0.05575736239552498\n","epoch6, iter50, loss: 0.06125881150364876\n","epoch6, iter60, loss: 0.07561621814966202\n","Finish epoch 6, time elapsed 11.102190732955933\n","---------iter=0\n","epoch6, pix_acc: 0.9664544677734375, meanIoU: 0.8360841103771115, IoUs: [0.91803734 0.96017392 0.63004107]\n","epoch7, iter0, loss: 0.09382818639278412\n","epoch7, iter10, loss: 0.0695514976978302\n","epoch7, iter20, loss: 0.09293340146541595\n","epoch7, iter30, loss: 0.07564447820186615\n","epoch7, iter40, loss: 0.12680469453334808\n","epoch7, iter50, loss: 0.06465194374322891\n","epoch7, iter60, loss: 0.060689959675073624\n","Finish epoch 7, time elapsed 11.265127897262573\n","---------iter=0\n","epoch7, pix_acc: 0.9494351196289063, meanIoU: 0.726314044079505, IoUs: [0.90601491 0.94186162 0.33106561]\n","epoch8, iter0, loss: 0.0838659480214119\n","epoch8, iter10, loss: 0.10764311999082565\n","epoch8, iter20, loss: 0.0947703942656517\n","epoch8, iter30, loss: 0.06307785958051682\n","epoch8, iter40, loss: 0.03601750358939171\n","epoch8, iter50, loss: 0.05752747505903244\n","epoch8, iter60, loss: 0.10346555709838867\n","Finish epoch 8, time elapsed 11.003725528717041\n","---------iter=0\n","epoch8, pix_acc: 0.966893310546875, meanIoU: 0.8473393989077707, IoUs: [0.91782051 0.96048104 0.66371665]\n","epoch9, iter0, loss: 0.06264064460992813\n","epoch9, iter10, loss: 0.036803245544433594\n","epoch9, iter20, loss: 0.06849616020917892\n","epoch9, iter30, loss: 0.0565461702644825\n","epoch9, iter40, loss: 0.05335857346653938\n","epoch9, iter50, loss: 0.05208710953593254\n","epoch9, iter60, loss: 0.06443364173173904\n","Finish epoch 9, time elapsed 11.203629970550537\n","---------iter=0\n","epoch9, pix_acc: 0.9689988708496093, meanIoU: 0.8522154670970283, IoUs: [0.91774187 0.96295578 0.67594875]\n","epoch10, iter0, loss: 0.05511414632201195\n","epoch10, iter10, loss: 0.044554777443408966\n","epoch10, iter20, loss: 0.051438573747873306\n","epoch10, iter30, loss: 0.07532444596290588\n","epoch10, iter40, loss: 0.06336084753274918\n","epoch10, iter50, loss: 0.03790230304002762\n","epoch10, iter60, loss: 0.04958255961537361\n","Finish epoch 10, time elapsed 11.129996538162231\n","---------iter=0\n","epoch10, pix_acc: 0.96635986328125, meanIoU: 0.8509810669775432, IoUs: [0.91944266 0.9595711  0.67392945]\n","epoch11, iter0, loss: 0.07307849824428558\n","epoch11, iter10, loss: 0.052567459642887115\n","epoch11, iter20, loss: 0.07577527314424515\n","epoch11, iter30, loss: 0.05138964205980301\n","epoch11, iter40, loss: 0.04064919427037239\n","epoch11, iter50, loss: 0.05711159482598305\n","epoch11, iter60, loss: 0.054333362728357315\n","Finish epoch 11, time elapsed 11.253590106964111\n","---------iter=0\n","epoch11, pix_acc: 0.968087158203125, meanIoU: 0.8544727036390368, IoUs: [0.91617247 0.96189759 0.68534805]\n","epoch12, iter0, loss: 0.08155134320259094\n","epoch12, iter10, loss: 0.053126346319913864\n","epoch12, iter20, loss: 0.053860146552324295\n","epoch12, iter30, loss: 0.07565822452306747\n","epoch12, iter40, loss: 0.04775882512331009\n","epoch12, iter50, loss: 0.05057882145047188\n","epoch12, iter60, loss: 0.060185015201568604\n","Finish epoch 12, time elapsed 11.073046445846558\n","---------iter=0\n","epoch12, pix_acc: 0.9702128601074219, meanIoU: 0.8480390324680593, IoUs: [0.92281178 0.96457817 0.65672714]\n","epoch13, iter0, loss: 0.07290875166654587\n","epoch13, iter10, loss: 0.06286624819040298\n","epoch13, iter20, loss: 0.0607045479118824\n","epoch13, iter30, loss: 0.05905066803097725\n","epoch13, iter40, loss: 0.061759259551763535\n","epoch13, iter50, loss: 0.04236597940325737\n","epoch13, iter60, loss: 0.06731574982404709\n","Finish epoch 13, time elapsed 11.222449541091919\n","---------iter=0\n","epoch13, pix_acc: 0.9676992797851562, meanIoU: 0.8264323500130194, IoUs: [0.92273007 0.96185009 0.59471689]\n","epoch14, iter0, loss: 0.04426256939768791\n","epoch14, iter10, loss: 0.049716878682374954\n","epoch14, iter20, loss: 0.051203131675720215\n","epoch14, iter30, loss: 0.05424055829644203\n","epoch14, iter40, loss: 0.060102786868810654\n","epoch14, iter50, loss: 0.06673717498779297\n","epoch14, iter60, loss: 0.058713290840387344\n","Finish epoch 14, time elapsed 11.251268863677979\n","---------iter=0\n","epoch14, pix_acc: 0.9635652160644531, meanIoU: 0.8423066409647387, IoUs: [0.9217348  0.95632345 0.64886168]\n","epoch15, iter0, loss: 0.060033075511455536\n","epoch15, iter10, loss: 0.05663605406880379\n","epoch15, iter20, loss: 0.07731737196445465\n","epoch15, iter30, loss: 0.05191933363676071\n","epoch15, iter40, loss: 0.06068935617804527\n","epoch15, iter50, loss: 0.04997657611966133\n","epoch15, iter60, loss: 0.05125071480870247\n","Finish epoch 15, time elapsed 11.094407081604004\n","---------iter=0\n","epoch15, pix_acc: 0.9699446105957031, meanIoU: 0.8444458957290016, IoUs: [0.92315153 0.96431055 0.64587561]\n","epoch16, iter0, loss: 0.049355264753103256\n","epoch16, iter10, loss: 0.040687400847673416\n","epoch16, iter20, loss: 0.05177374556660652\n","epoch16, iter30, loss: 0.06971561163663864\n","epoch16, iter40, loss: 0.048276275396347046\n","epoch16, iter50, loss: 0.04828270152211189\n","epoch16, iter60, loss: 0.0466865636408329\n","Finish epoch 16, time elapsed 11.171679019927979\n","---------iter=0\n","epoch16, pix_acc: 0.9698100280761719, meanIoU: 0.8543421709611035, IoUs: [0.91892088 0.96397179 0.68013384]\n","epoch17, iter0, loss: 0.09245619922876358\n","epoch17, iter10, loss: 0.05405227467417717\n","epoch17, iter20, loss: 0.047312043607234955\n","epoch17, iter30, loss: 0.037820711731910706\n","epoch17, iter40, loss: 0.03870638087391853\n","epoch17, iter50, loss: 0.044131089001894\n","epoch17, iter60, loss: 0.05891082063317299\n","Finish epoch 17, time elapsed 11.128046989440918\n","---------iter=0\n","epoch17, pix_acc: 0.9715863037109375, meanIoU: 0.8657153181355058, IoUs: [0.92457875 0.96597577 0.70659143]\n","epoch18, iter0, loss: 0.05189446732401848\n","epoch18, iter10, loss: 0.04832884669303894\n","epoch18, iter20, loss: 0.0527145117521286\n","epoch18, iter30, loss: 0.04008498415350914\n","epoch18, iter40, loss: 0.04256226122379303\n","epoch18, iter50, loss: 0.04405798017978668\n","epoch18, iter60, loss: 0.05651969462633133\n","Finish epoch 18, time elapsed 11.116293668746948\n","---------iter=0\n","epoch18, pix_acc: 0.9734808349609375, meanIoU: 0.8612947063489532, IoUs: [0.92424581 0.968396   0.69124231]\n","epoch19, iter0, loss: 0.056725163012742996\n","epoch19, iter10, loss: 0.04087211564183235\n","epoch19, iter20, loss: 0.03751150891184807\n","epoch19, iter30, loss: 0.036410290747880936\n","epoch19, iter40, loss: 0.04678906500339508\n","epoch19, iter50, loss: 0.04794973134994507\n","epoch19, iter60, loss: 0.05294587090611458\n","Finish epoch 19, time elapsed 11.194506406784058\n","---------iter=0\n","epoch19, pix_acc: 0.9724794006347657, meanIoU: 0.8611831778452875, IoUs: [0.92292411 0.96719853 0.69342689]\n","The highest mIOU is 0.8657153181355058 and is achieved at epoch-18\n","The highest pixel accuracy  is 0.9734808349609375 and is achieved at epoch-19\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IbSvzMUrlGZU","colab_type":"code","outputId":"609afd2f-1d04-43d8-b67e-4ce30f8748fa","executionInfo":{"status":"ok","timestamp":1576771098424,"user_tz":-480,"elapsed":453340,"user":{"displayName":"張聿程","photoUrl":"","userId":"08620233356790329924"}},"colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["epoch = list(range(len(pixel_acc_list)))\n","plt.plot(epoch, pixel_acc_list, epoch, mIOU_list)\n","plt.legend(['pixel_acc', 'mIOU'])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f566ccd50b8>"]},"metadata":{"tags":[]},"execution_count":14},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXQAAAD6CAYAAACxrrxPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXyU1b348c83kz0EQhYWCZCg7CBV\nIlCxbigC9aqt7S1WW7VVbrW0ve3trdjF9traXmvb2+st1nKtW63F5XfbUkRRXItFJSgqS4CwCAlL\nJiEkkIUs8/39cSZhCAmZJDOZmeT7fr3m9Wxn5vnmmZlvzpznnOcRVcUYY0zsi4t0AMYYY0LDErox\nxvQRltCNMaaPsIRujDF9hCV0Y4zpIyyhG2NMH9FpQheRh0WkTEQ2dbBdROR+ESkWkQ9E5NzQh2mM\nMaYz8UGUeRT4DfB4B9vnA2P9j5nAb/3T08rOzta8vLyggjTGGONs2LChXFVz2tvWaUJX1TdEJO80\nRa4GHlc3QuktEckQkeGqeuB0r5uXl0dhYWFnuzfGGBNARD7qaFso2tBHAPsClkv864wxxvSiXj0p\nKiKLRKRQRAq9Xm9v7toYY/q8UCT0UmBkwHKuf90pVHWZqhaoakFOTrtNQMYYY7opFAl9BfBFf2+X\nWUBVZ+3nxhhjQq/Tk6Ii8ifgYiBbREqAHwIJAKr6ILAKWAAUA7XAzeEK1hhjTMeC6eVyXSfbFfhq\nyCIyxhjTLTZS1Bhj+ohgBhYZY0yf1Njso76xmfpGNz3edGK+dXrSOvdoaFY8IsR7hASPEB8XR3zA\ntGVdgkfwtKwL2DYqM42c9KSQ/z2W0E2/1exT6hubqWtspq6huXW+oclHcoKHAUnxpCa5aUqCBxEJ\ne0w+n1Lf5OJpiStweiJeH7UNTSct+1RJS/KQmhhPWqKH1KR40hLd35CWGE9qooe0pBPbUhM8xMWF\n/28K1Njs42BVPfuP1LG/qo79R+o53uQDQICWQyxIwHzA+oD3QASam93xOt7oa50eb2pJzidPG9os\n1zf5aPZF5o5tP75mCl+YNTrkr2sJ3YRcxbHjbNpfzdYD1YzISOGSCUMYkNQ7H7XSI3W8sOkg7+87\nQm1Dc7sJu66xmdoG9wUPlgikJcaT5k+OaUntz6cmxePzaWtN73iTSyAnJ5yTk43b7hJMV2JqESeQ\nmhiPALWNzV1KUikJHhd7UjwZqYlkpiYwODWRwWmJZKYlkpGaQKZ/2a132xM87bfWHq1vpPRIHfuP\n1FF6pJ7SypZ5Nz1UXU+oc2icQHKCh+QED0nxca3TJP90YEoCOelJJ9YHlElO8JCS4CE5wZVPTvCQ\n7F+f7F/v1gWWiSPRE4dP3T+oJp/S3Kw0+nw0NWvruqbW6YltTc0+Gn3KWUMGhPYg+FlC7+Mam31U\n1jac9kvYXarKoerjbCqtYtP+KjaVVrN5fxUHqupPKpfoiWP2WVlcMXkYl00aSvaA0P7U3FNew/Ob\nDvLCpgO8X1IFwIiMFAalJJCS6L6wg1MT/fNx7gvsX5+S4CEl0dP6xU5J8JAQH0ddQzO1DU3UHG+i\npqHZTY+76bGGJmr9ywer608qU9vQfEqCcQ9/Qoh3yTMz7UTCSYpvSUBx7caU6o+1o5gTPNJac1VV\njjf5qA2Ip6ahidrj7u8JXK5pWT7exLHjTVTWNlJ+rIHth45RWdtAbUNzh8c8PSneJfm0RNKT4ik/\ndpzSI3UcrW86qVyCRxg+KIUzMpL5+JlZ5GakcEZGCiMGu+kZg1JISfS0xu6m0JLzVTVgHlqWWm6F\n7ImTkH+ug+UR8MR5IrLvjkikbhJdUFCgdi2X0PD5lAPV9ewpr2FXeQ27vTXsqahhd3kN+w7X0uRT\nRCArLYlhg5IYmp7MkIHJDBuYzNCBSQwdlMzQdDefmZbYbtOCqlJSWXdK8i4/1gC4GuyY7DSmjBjE\nlDMGMXnEQCYOG8iOsmOs3nyQ1ZsPUlJZR5xAwehM5k4eyhWThzEyM7Vbf/OOQ0d5ftNBVn14gKKD\nRwE4O3cQ86YMY/6U4eRnp3X/gPaAqvZK00xvqG9s5khtI4drGjhS28Dh2gYqaxo4XNNIZW0DlbUN\nHK5p4Gh9E9kDEl2iDkjYIzJSyB6QhKeXm3X6OhHZoKoF7W6zhB4bVJWKmgZ2l9eceAQk7uMBP9VT\nEjzkZacxJjuNvOxUhg5MpuJYA2VH6zlYVc+h6uMcqq6noqbhlP0keuLISU9i2CCX4DNSE9lTXsOm\n0iqq/bUvT5wwdsgAf/IeyJQRg5g4fCBpp2lWUVW2HKhm9eZDvLj5YGsSnjR8IPOmDOOKycMYN3RA\nh8lQVdm8v5oXNh3k+U0H2OmtAaBg9GDmTRnGvCnDyB3cvX8OxsQSS+gxpL6xmd3lNezy1rDLe4xd\n5SemgT9nEzzCyMxUxmSnkZ+dRp5/OiZ7AEMHJgVVS2xo8lF21CX4sup6DlafSPaH/MuHaxoYlZnK\n5DMGMWXEQKacMYjxw9JJTujZT8095TW8uOUgqzcf4t29lahCXlYqV0wextzJwzhnZAYisHHfEX8S\nP8jew7XECczMz2L+VPdPYOjA5B7FYUyssYQeZVSVA1X1LmmXH2OXt4adXjfdX1VH4FtyxqBkxuQM\nIN+fsPNzXM17REYK8RFqOwy1sup6XtxyiNWbD7JuZwVNPmVIehLxccL+qnoSPML5Z2Yzf8owLp80\nlKwQt8EbE0ssoUfY4ZoG/vJeKe/urWSX1zWR1DWeOOGUluhhTM4AxuS4GvaYnDTG5LgEnprYv85b\nV9U18mpRGS9uOUizT5k7aRiXTRzKoNSESIdmTFQ4XULvX9miF6kq63ZV8Kd39rF600Eamn2MzEzh\nzJwBzBqT1Zq0z8wZwJD04JpI+oNBKQlcc84IrjnHLqlvTFdZQg+x8mPHeXZDCcvf2cueiloGJsfz\n+ZmjWDhjJBOGDYx0eMaYPswSegj4fMra4nKWr9/Li5sP0eRTZuRl8vU5Y1kwdXiPTyAaY0wwLKH3\nwKHqep4p3MdThfvYd7iOwakJ3HR+HgtnjOSsIemRDs8Y089YQu+iZp/yxnYvT76zl1eKymj2KR8f\nk8W/XzGBKyYPJSneauPGmMiwhB4kVeX3a3fz8Nrd7K+qJ3tAIrd+YgyfO29kxEYlGmNMIEvoQXp9\nu5efPLeVmfmZ/ODKScyZOJTE+L7RD9wY0zdYQg9Cs0/52aoiRmel8ocvz7REboyJSpaZgvDshn1s\nO3SU71wxwZK5MSZqWXbqRG1DE798cTvnjMpgwdRhkQ7HGGM6FFRCF5F5IrJNRIpFZEk720eLyMsi\n8oGIvCYiuaEPNTIe+vtuyo4e53sLJtpoTmNMVOs0oYuIB1gKzAcmAdeJyKQ2xX4BPK6qZwN3Az8L\ndaCR4D16nN+9vpN5k4dRkJcZ6XCMMea0gqmhzwCKVXWXqjYAy4Gr25SZBLzin3+1ne0x6ddrtnO8\nyccd8ydEOhRjjOlUMAl9BLAvYLnEvy7Q+8Cn/fOfAtJFJKvtC4nIIhEpFJFCr9fbnXh7TXHZUZav\n38f1M0dZP3NjTEwI1UnRbwMXich7wEVAKXDKDQlVdZmqFqhqQU5OToh2HR7/+XwRqQkevj5nbKRD\nMcaYoATTD70UGBmwnOtf10pV9+OvoYvIAOBaVT0SqiB721u7KliztYzvzBtvN1MwxsSMYGro64Gx\nIpIvIonAQmBFYAERyRaRlte6E3g4tGH2Hp9P+emqrZwxKJkvzc6PdDjGGBO0ThO6qjYBi4HVwFbg\naVXdLCJ3i8hV/mIXA9tEZDswFLgnTPGG3d8+2M8HJVX829zxdtlbY0xMCWrov6quAla1WXdXwPyz\nwLOhDa331Tc28/MXtjFp+EA+ZXfMMcbEGBspGuDxdXsoPVLHdxdMJC7OBhEZY2KLJXS/I7UN/OaV\nYi4en8MFY7MjHY4xxnSZJXS//3mlmGPHm7hz/sRIh2KMMd1il88F9lbU8vi6PXx2+kjGD7NbxxnT\n5zTWw6FNUFIIpRvc4+gBSMmEtCxIzYa0bP+07XI2pGZB8iCI8us5WUIHfr66iPi4OL41d1ykQzHG\n9JTPBxU7TiTu0g1wcBP4Gt32AcMgtwDGzYO6Sqgth5py95yaCmisaf914xJOJPmUDIhr6QXnT/Kt\nyT6I5fNugXFzQ/c3+/X7hP7e3kpWfnCAr196FkMHJkc6HGNMV1UfODl5738Pjle7bYkD4Ixz4ONf\nhRHT3WNQJz3YGutcgm9J9IHzteUu6ddVQnMDqPqf5J8Gu9xYG4I//FT9OqGrukFE2QOSWHTRmZEO\nx0Saz+e+qHWHQeLAk+h/JJw8H+zPbp/PJZb6I1BfBXVH2sxXueXW+SpISIFBIyFjJAzKPTE/cATE\n9/NRyz4fHNnjatuHNrnp/vfg6H63PS4ehk6GqZ+BEQUueWePDahJBykhxR3zjJGdl40y/Tqhv7jl\nEOv3VHLPp6YwIKlfH4q+y+dzCfpYGdSUwTEvHDt0Yr6mzG07VuZqX76mzl8zLr79RO9JdNsaavyJ\nu5rWmll7xOPaZZMHuZ/wyYOgoRZ2vgxHD7Z5rsCAoS7JZ4x0ib418fuTf0pGDw9WFGmohbItcPDD\nE8n70GZoOOq2SxxknQWjz3fNJyOmw7CpLhn3Y/02izU2+7j3+SLOGjKAzxXE3n9icxof/QNeuNMl\nxRov6CnXiXPtoQOGuEf6cBh+tkuYaUMgNdP9NG5u8D8aO5lvdO2zLfMJqf4EnRGQrNuZT0rvuLbf\n1ADVpVC1D47sg6oSqNrr5g98AEWroPn4yc8ZNArO/QKc8wUYODz0xzUcVKF6vz9pf3hiWrGT1n9o\niekwbApMW+imQ6fCkImQmBrR0KNRv03oy9/Zy67yGh76YgHxHuu92WeowgtL4OghGHvZiSQ9IMc/\nHermkzOiu8dCfCJk5rtHe3w+94viyL4TiX7ny/DqPfDaf8L4+TD9ZjjzUogL4+f7mBc+etM1FzXW\n+h917tFQ458PWNfYZl1DzcntyRmjXU17ymdc8h421a2L5vcqivTLhH60vpFfr9nBzPxM5kwcEulw\nTCh99CYceB/+6b9h+k2RjiZ84uJO/MLIne7Wzf66q9m++xi890coWgkZo+DcG12tPX1oz/erCgc/\ngO0vwvYX3EnIts1KEud+pSSkuiaQwGlazqnrWpL40Enul4vptn6Z0B98fScVNQ088km7T2jYNDdB\n4e9h1CwYPq339rtuqeszfPbnem+f0STrTLj8brjkey6hFz4Cr/wYXvsZjF8ABTdD/sVdq7U31MCu\n12HHapfIW05CjpgOl3wXzprjmq1aErQn0WrUEdLvEvqBqjoe+vturv7YGZyd24dOIkWTY1549mbY\n83cYOgX+5e/h/dnfomInbHseLvz3fn9yjPgkmHKte5QXw4ZHYOOTsHUFDM5zv14+dr2r4ben8iPY\n8SJsXw2733Dt9YkDXBPOuHkw9vKOn2sipt8l9F++uB1V+Pbc8ZEOpW/atx6e/qLrWXL2QvhgOWx/\nHiZ8Mvz7fuu3rrfJebeEf1+xJPssuOIeuPQHsPVvLrmv+RG8co97XwpuhtGz3SjK7S+4RF62xT03\ncwyc92UYdwWMOt+17ZuoFXMJ/ZE3d/NfL23v9vOr65tYdOEYRmb2gzPk9VXw2r2w8xWY+S+uLTVc\nNWVV18Ty/BI3cOPLL8GQSbDvbXj9XvdzP5w/w2sPw8Y/wtTPhqatuC9KSIazP+se3m2w4VFXa9/y\nF9dM0tzgul2O+jjMvcfVxLPPinTUpgtiLqGPG5rOp8/N7fbzM1ITuOUTY0IYURTy+eD9J10trKYc\ncsbDyn+F956AK38V+jbtxjpY+S23z7Fz4dPLIGWw2/aJf4MVi2HHS2EZ6tzq3cdcb4lZt4dvH31J\nzniY9zOYcxds+aurnefNdk0qdmIyZonqaQY+hFFBQYEWFhZGZN99WskGeP7fXe+DkTNh/s9dAv/g\naXjxe1BbATMWuZNmyQN7vr/KPfDUF1zPh4uWwEV3nPwroLkR7j/XtbfesiY8tfSmBvjvsyF7HNy4\novPyxsQwEdmgqgXtbbMO2H3FsTL461fhoUuhqhQ+tQy+tBrO+JhLotM+B4sLoeBL8Pbv4DfnwYfP\nBlxrohuK18DvLoIjH8Hnn4ZL7jy1SceTAJ/4JpQWwq5Xe/Y3dmTLX9yV8z6+ODyvb0yMsIQe65ob\nYd0D8D/T4f2n4Pyvw9cKXQJvWxtOyYBP/hJufQXSh8H/+zI8fjWU7+jaPn0+eP0+eOIzbsj5otfc\nSbOOfOx6dy2S1+/r6l/XOVVY9xtXOz/rstC/vjExJKiELiLzRGSbiBSLyJJ2to8SkVdF5D0R+UBE\nFoQ+VHOKXa/BgxfA6jth5Ay4fR3M/bEbUn46I851SX3BL2D/Rvjt+fDKT1xbeGfqjsDyz8OrP4Gz\n/9md/Mzs5JxEfBLM/gbs/QfsWRv0nxeUj/7hBhLNuq13ukYaE8U6/QaIiAdYCswHJgHXicikNsW+\nDzytqucAC4EHQh2oCVD5ETx1g6tdN9XDwj/B9c+6K8sFK84DM251tfnJn4I37oOlM12/444c2gz/\newkUvwTz74NP/S7462mc+0U37P71nwcfYzDeesDdpODshaF9XWNiUDBVmhlAsaruUtUGYDlwdZsy\nCrScYRsE7A9diKZVY527TsfSGVD8Mlz6fbj9bZjQgy6BA4a4Xik3roT4ZHjyn2H59e7aIIE+eAb+\nd467Ct5Nz8HMRV3bZ0KKaw7a/Trsfbt7sbZVsROKnnP9pO1CTcYEldBHAIHf7hL/ukA/Am4QkRJg\nFfC19l5IRBaJSKGIFHq93m6E20+pwpYV8JsZJ4ZwL17vHxEZopty5H8CvrIWLvuR+2exdAas/bVL\n4M/fAf93i7tRwL+84Ybzd0fBzW5Y/hshqqW//aDrN20DiYwBQndS9DrgUVXNBRYAfxCRU15bVZep\naoGqFuTk5IRo131YU4OrGT80B57+gmsbv+k5+Owj7mRkqMUnwgXfhMXvwJhLYM0P4b6zXOKceZvr\nEtiTQTuJaXD+11zvmNINPYu1rtJdgGrqZ90JXmNMUAm9FAi8YHiuf12gLwNPA6jqOiAZyA5FgP3S\nsTI3wvPXU1zNuL4KrvwvVzvOuyD8+88YBdc9Cdctd9cJv/b3MP8/XRfEnjrvFjfoqKc9XjY85i7F\n+nEbSGRMi2BGiq4HxopIPi6RLwQ+36bMXmAO8KiITMQldGtT6arSd10f8c3/54Zhn3U5zPoKjAnz\nNa07Mn6+e4RSUrobzfnqPe5GDcPP7vprNDe645R/obvsqjEGCCKhq2qTiCwGVgMe4GFV3SwidwOF\nqroC+Dfgf0Xkm7gTpDdppIagxprmRncFvLcehJJ33BXtpt/sRnP21etozFgE//gf17Pmc3/o+vO3\n/NVdwvWffh362IyJYUFdy0VVV+FOdgauuytgfgswO7Sh9XE15e6qd+t/70Y5Zo6BeffCxz4fmiH5\n0SwlA2Z+xZ0cPbTF3dggWC0DibLGul8wxphWMXdxrph34H14exl8+Iy7xvSZl7q765x1ef8aGDPr\nNteH/O+/gM88HPzz9q5zd3r/5K/61/EyJgiW0HvLR+vg5bvdaMmENHcz3xmL3FXv+qPUTHeC9M3/\nhovvDH5Q1Lql7qTqtOvCG58xMciqOL3B54NnbnJXJpx7D3xri7umSn9N5i0+vtgNOPr7L4Mrf3iX\nG0hUYAOJjGmPJfTeUFoIxw7C5f8B5y92bcgGBuS4qz9+8LRL1p15yz+QaMat4Y/NmBhkCb03FK10\niWhsGG/wEKvO/5o7Nn//1enL1R1xN+iY+hkbSGRMByyhh5sqbF3p+kxbzfxU6cNg+o3w/p/gyN6O\ny73rH0hkdyQypkOW0MPNuw0O7+ydmyTHqtn/ChIHa/+r/e0tA4nyPtG9gUjG9BOW0MOt6G9uOt4S\neocGjXA3wXjvCahu50KdW/4K1aV2RyJjOmEJPdy2roQRBTBweKQjiW4XfBPU57oxBmodSHSWnYMw\nphOW0MOpqgQObISJV0Y6kug3eDRMWwgbHoWjh06s3/uWG0hkdyQyplP2DQmnoufcdMI/RTaOWHHB\nt9xFyf5x/4l1b9lAImOCZQk9nIpWQvb4vnuRrVDLOtNd37zwYXetm8O7XJPV9JvdtdSNMadlCT1c\nag/DnjetuaWrPvFtd6u9dUtdz5a4eHeJBGNMp+xaLuGyfTVos3VX7Kqcce6m1e8sc8tTrrUTysYE\nyWro4VK0EtLPgDPOjXQksefCb0PDMfewOxIZEzSroYdDQ6270fI5N4BIpKOJPUMnu2NXXwXDp0U6\nGmNihiX0cNj5CjTVWft5T1y9NNIRGBNzrMklHIpWQnIGjLabOBljeo8l9FBrboJtz8O4eeBJiHQ0\nxph+JKiELiLzRGSbiBSLyJJ2tv+XiGz0P7aLyJHQhxojPnoT6o9Yc4sxptd12oYuIh5gKXA5UAKs\nF5EV/htDA6Cq3wwo/zXgnDDEGhuKnoP4ZHevUGOM6UXB1NBnAMWquktVG4DlwNWnKX8d8KdQBBdz\nVF1CP3OOjWw0xvS6YBL6CGBfwHKJf90pRGQ0kA+80vPQYtCBjVBdYoOJjDEREeqToguBZ1W1ub2N\nIrJIRApFpNDr9YZ411Fg60oQD4yfH+lIjDH9UDAJvRQYGbCc61/XnoWcprlFVZepaoGqFuTk5AQf\nZawoWgmjz4fUzEhHYozph4JJ6OuBsSKSLyKJuKS9om0hEZkADAbWhTbEGFFeDN4imGC9W4wxkdFp\nQlfVJmAxsBrYCjytqptF5G4RuSqg6EJguapqeEKNckUr3dTaz40xERLU0H9VXQWsarPurjbLPwpd\nWDGo6Dl33ZGMkZ2XNcaYMLCRoqFw9CCUvGN3JjLGRJQl9FDY5v/xYqNDjTERZAk9FLauhMwxkDMh\n0pEYY/oxS+g9VV8Fu99wvVvs2ufGmAiyhN5TO14CX6N1VzTGRJwl9J7a+jdIGwK550U6EmNMP2cJ\nvSca66F4DUxYAHF2KI0xkWVZqCd2v+5uZGzdFY0xUcASek8UrYSkgZB/YaQjMcYYS+jd5muGolUw\n9nKIT4x0NMYYYwm92/a9DbXl1rvFGBM1LKF3V9Fz4EmEsy6LdCTGGANYQu8eVdddcczFkDww0tEY\nYwxgCb17Dm2GIx/ZpXKNMVHFEnp3FK0EBMYviHQkxhjTyhJ6dxSthFGzYMCQSEdijDGtLKF3VeUe\nOPihNbcYY6KOJfSuKnrOTS2hG2OijCX0rip6DoZMdtc/N8aYKBJUQheReSKyTUSKRWRJB2X+WUS2\niMhmEXkytGFGiZpy2LvO7kxkjIlKnd4kWkQ8wFLgcqAEWC8iK1R1S0CZscCdwGxVrRSRvnm2cNvz\noD4bHWqMiUrB1NBnAMWquktVG4DlwNVtytwKLFXVSgBVLQttmFGiaCUMGgXDpkY6EmOMOUUwCX0E\nsC9gucS/LtA4YJyIvCkib4nIvFAFGDWajsPOV93JULvVnDEmCnXa5NKF1xkLXAzkAm+IyFRVPRJY\nSEQWAYsARo0aFaJd95KKYmg+DrkFkY7EGGPaFUwNvRQYGbCc618XqARYoaqNqrob2I5L8CdR1WWq\nWqCqBTk5Od2NOTK8RW6aMz6ycRhjTAeCSejrgbEiki8iicBCYEWbMn/B1c4RkWxcE8yuEMYZed5t\nIHGQdcr/KWOMiQqdJnRVbQIWA6uBrcDTqrpZRO4Wkav8xVYDFSKyBXgV+HdVrQhX0BFRthUG50NC\ncqQjMcaYdgXVhq6qq4BVbdbdFTCvwLf8j77Juw1yJkQ6CmOM6ZCNFA1GUwMc3mnt58aYqGYJPRiH\nd4GvCYZMjHQkxhjTIUvowfBudVOroRtjopgl9GB4twFiPVyMMVHNEnowvEUweDQkpkY6EmOM6ZAl\n9GBYDxdjTAywhN6Z5iYo32EJ3RgT9Syhd+bwLvA1WkI3xkQ9S+idsWu4GGNihCX0zni3uWn2uMjG\nYYwxnbCE3hlvEWSMgqQBkY7EGGNOyxJ6Z6yHizEmRlhCPx1fM5Rvt/ZzY0xMsIR+OpV73F2KrIZu\njIkBltBPp7WHi12UyxgT/Syhn05rQrceLsaY6GcJ/XTKimBgLiSlRzoSY4zplCX00/EW2QlRY0zM\nsITekdYeLnZC1BgTGyyhd+TIXmiqhyGW0I0xsSGohC4i80Rkm4gUi8iSdrbfJCJeEdnof9wS+lB7\nWcuQf6uhG2NiRHxnBUTEAywFLgdKgPUiskJVt7Qp+pSqLg5DjJHRcts5u4aLMSZGBFNDnwEUq+ou\nVW0AlgNXhzesKODdBunDISUj0pEYY0xQgknoI4B9Acsl/nVtXSsiH4jIsyIysr0XEpFFIlIoIoVe\nr7cb4fYib5E1txhjYkqoTor+DchT1bOBl4DH2iukqstUtUBVC3JyckK06zDw+cBrPVyMMbElmIRe\nCgTWuHP961qpaoWqHvcvPgRMD014EVJdAo011gfdGBNTgkno64GxIpIvIonAQmBFYAERGR6weBWw\nNXQhRkBZy5B/q6EbY2JHp71cVLVJRBYDqwEP8LCqbhaRu4FCVV0BfF1ErgKagMPATWGMOfzstnPG\nmBjUaUIHUNVVwKo26+4KmL8TuDO0oUWQdxsMGAqpmZGOxBhjgmYjRdtj13AxxsQgS+htqdpt54wx\nMckSelvVpdBw1GroxpiYYwm9LbtLkTEmRllCb8suymWMiVGW0NvyFkFqNqRlRToSY4zpEkvobZXZ\nNVyMMbHJEnqg1h4udkLUGBN7LKEHOnoQjlfBEDshaoyJPZbQA9mQf2NMDLOEHsh6uBhjYpgl9EDe\nrZAyGNKi+FrtxhjTAUvogbzb3IAikUhHYowxXWYJvYUqlG219nNjTMyyhN6ixgv1R6z93BgTsyyh\ntyjz32TJaujGmBhlCb2F9XAxxsQ4S+gtvEWQPAjSh0U6EmOM6RZL6C1abmphPVyMMTEqqIQuIvNE\nZJuIFIvIktOUu1ZEVEQKQhdiL7HbzhljYlynCV1EPMBSYD4wCbhORCa1Uy4d+AbwdqiDDLuacqgt\nt/ZzY0xMC6aGPgMoVtVdqkmZfJIAAA54SURBVNoALAeubqfcj4F7gfoQxtc7Wq/hYgndGBO7gkno\nI4B9Acsl/nWtRORcYKSqPhfC2HqPJXRjTB/Q45OiIhIH/Ar4tyDKLhKRQhEp9Hq9Pd116Hi3QWI6\nDDwj0pEYY0y3BZPQS4GRAcu5/nUt0oEpwGsisgeYBaxo78Soqi5T1QJVLcjJiaILYLUM+bceLsaY\nGBZMQl8PjBWRfBFJBBYCK1o2qmqVqmarap6q5gFvAVepamFYIg6Hli6LxhgTwzpN6KraBCwGVgNb\ngadVdbOI3C0iV4U7wLCrPQw1ZTDEEroxJrbFB1NIVVcBq9qsu6uDshf3PKxeZEP+jTF9hI0UtdvO\nGWP6iKBq6H2atwgS0mBgbqQjMabPaGxspKSkhPr62BuWEi2Sk5PJzc0lISEh6OdYQm8Z8h9nP1aM\nCZWSkhLS09PJy8tDrPdYl6kqFRUVlJSUkJ+fH/TzLItZDxdjQq6+vp6srCxL5t0kImRlZXX5F07/\nTuh1R+DoAWs/NyYMLJn3THeOX/9O6OXb3dRq6MaYPqB/J3S77Zwx/c4tt9zCli1buvXcvLw8ysvL\nQxxR6PTvk6LebRCfAhmjIx2JMaaXPPTQQ5EOIWz6eUIvgpxx1sPFmDD6j79tZsv+6pC+5qQzBvLD\nf5p82jJ79uxh3rx5TJ8+nXfffZfJkyfz+OOPs2DBAn7xi1+Qk5PDZZddxrp168jMzOSiiy7iBz/4\nAXPnzuWJJ57g/vvvp6GhgZkzZ/LAAw/g8Xg6jeuaa65h37591NfX841vfINFixYB8MILL/Dd736X\n5uZmsrOzefnllzl27Bhf+9rXKCwsRET44Q9/yLXXXtuj49LPE/o2yJsd6SiMMWGybds2fv/73zN7\n9my+9KUv8cADD7RuGz16NHfccQe33XYbM2bMYNKkScydO5etW7fy1FNP8eabb5KQkMDtt9/OH//4\nR774xS92ur+HH36YzMxM6urqOO+887j22mvx+XzceuutvPHGG+Tn53P48GEAfvzjHzNo0CA+/PBD\nACorK3v89/bfhF5fDdUl1n5uTJh1VpMOp5EjRzJ7tqu03XDDDdx///0nbb/lllt45plnePDBB9m4\ncSMAL7/8Mhs2bOC8884DoK6ujiFDhgS1v/vvv58///nPAOzbt48dO3bg9Xq58MILW/uTZ2ZmArBm\nzRqWL1/e+tzBgwf34C91+m9Cb+3hMjGycRhjwqZt17+2y7W1tZSUlABw7Ngx0tPTUVVuvPFGfvaz\nn3VpX6+99hpr1qxh3bp1pKamcvHFF/f6SNn+23hs13Axps/bu3cv69atA+DJJ5/kggsuOGn7HXfc\nwfXXX8/dd9/NrbfeCsCcOXN49tlnKSsrA+Dw4cN89NFHne6rqqqKwYMHk5qaSlFREW+99RYAs2bN\n4o033mD37t2trwdw+eWXs3Tp0tbnh6LJpX8ndE8SDM6LdCTGmDAZP348S5cuZeLEiVRWVnLbbbe1\nbnv99ddZv359a1JPTEzkkUceYdKkSfzkJz9h7ty5nH322Vx++eUcOHCg033NmzePpqYmJk6cyJIl\nS5g1axYAOTk5LFu2jE9/+tNMmzaNz33ucwB8//vfp7KykilTpjBt2jReffXVHv+9oqo9fpHuKCgo\n0MLCCN4D44+fheoDcNvayMVgTB+1detWJk6MbHPmnj17uPLKK9m0aVNE4+iJ9o6jiGxQ1VPuCAf9\nuYZeVmTNLcaYPqV/nhQ9fgyq9sL0zrshGWNiU15eXlhq5xUVFcyZM+eU9S+//DJZWVkh319X9M+E\nbtdwMcZ0U1ZWVmsXx2jTP5tc7LZzxpg+KKiELiLzRGSbiBSLyJJ2tn9FRD4UkY0islZEJoU+1BDy\nFoEnEQYHf+F4Y4yJdp0mdBHxAEuB+cAk4Lp2EvaTqjpVVT8G/Bz4VcgjDSVvEWSNBU//bHEyxvRN\nwdTQZwDFqrpLVRuA5cDVgQVUNfDKO2lAZPpCBstrPVyMMfDoo4+yePHi1uVly5YxYcIEJkyYwIwZ\nM1i79kS35raXzn3ttde48sorezXezgRTRR0B7AtYLgFmti0kIl8FvgUkApeGJLpwaKiFyo9g2ucj\nHYkxJoqsXLmS3/3ud6xdu5bs7GzeffddrrnmGt555x2GDRsW6fCCErKToqq6VFXPBO4Avt9eGRFZ\nJCKFIlLo9XpDteuuqdgBqNXQjenj9uzZw4QJE7jpppsYN24c119/PWvWrGH27NmMHTuWd95556Ty\n9957L/fddx/Z2dkAnHvuudx4440nDc+PdsHU0EuBkQHLuf51HVkO/La9Daq6DFgGbqRokDGGVpn/\nGi5D7KJcxvSK55fAwQ9D+5rDpsL8/+y0WHFxMc888wwPP/ww5513Hk8++SRr165lxYoV/PSnP+Wa\na65pLbt582amT59+0vMLCgp47LHHQht7GAVTQ18PjBWRfBFJBBYCKwILiMjYgMVPAjtCF2KIeYsg\nLh4yx0Q6EmNMmOXn5zN16lTi4uKYPHkyc+bMQUSYOnUqe/bs6dJrtXfT5mi7EXanNXRVbRKRxcBq\nwAM8rKqbReRuoFBVVwCLReQyoBGoBG4MZ9A94t0GWWeBJyHSkRjTPwRRkw6XpKSk1vm4uLjW5bi4\nOJqamk4qO2nSJDZs2MCll544BbhhwwYmT3bXc8/KyqKysrK1Sebw4cOt89EiqDZ0VV2lquNU9UxV\nvce/7i5/MkdVv6Gqk1X1Y6p6iapuDmfQPWI9XIwx7fjOd77DHXfcQUVFBQAbN27k0Ucf5fbbbwfg\n4osv5g9/+AMAzc3NPPHEE1xyySURi7c9/asjdmM9VO6GqZ+JdCTGmChz1VVXUVpayvnnn4+IkJ6e\nzhNPPMHw4cMB+MEPfsBtt93GtGnTUFXmzZvHDTfcEOGoTxZ7l8999w+w7jfd22lzIxzeCZ95BKZ8\nunuvYYzpVDRcPrcv6Orlc2Ovhp6a2bMmk5Ez4czo+plkjDGhEHsJfcIn3cMYY8xJ+ufVFo0xpg+y\nhG6MCYtInZ/rK7pz/CyhG2NCLjk5mYqKCkvq3aSqVFRUkJyc3KXnxV4bujEm6uXm5lJSUkLErtnU\nByQnJ5Obm9ul51hCN8aEXEJCAvn5dgOZ3mZNLsYY00dYQjfGmD7CEroxxvQRERv6LyJe4KNuPj0b\nKO+0VO+zuLrG4uq6aI3N4uqansQ1WlVz2tsQsYTeEyJS2NG1DCLJ4uoai6vrojU2i6trwhWXNbkY\nY0wfYQndGGP6iFhN6MsiHUAHLK6usbi6Llpjs7i6JixxxWQbujHGmFPFag3dGGNMG1Gd0EVknohs\nE5FiEVnSzvYkEXnKv/1tEcnrhZhGisirIrJFRDaLyDfaKXOxiFSJyEb/465wx+Xf7x4R+dC/z1Nu\nByXO/f7j9YGInNsLMY0POA4bRaRaRP61TZleO14i8rCIlInIpoB1mSLykojs8E8Hd/DcG/1ldohI\nyG6E3kFM94lIkf99+rOIZHTw3NO+52GK7UciUhrwfi3o4Lmn/f6GIa6nAmLaIyIbO3huWI5ZR7mh\nVz9fqhqVD8AD7ATGAInA+8CkNmVuBx70zy8EnuqFuIYD5/rn04Ht7cR1MbAyAsdsD5B9mu0LgOcB\nAWYBb0fgPT2I60cbkeMFXAicC2wKWPdzYIl/fglwbzvPywR2+aeD/fODwxjTXCDeP39vezEF856H\nKbYfAd8O4r0+7fc31HG12f5L4K7ePGYd5Ybe/HxFcw19BlCsqrtUtQFYDlzdpszVwGP++WeBOSIi\n4QxKVQ+o6rv++aPAVmBEOPcZQlcDj6vzFpAhIsN7cf9zgJ2q2t0BZT2mqm8Ah9usDvwcPQZc085T\nrwBeUtXDqloJvATMC1dMqvqiqjb5F98CunbZvRDp4HgFI5jvb1ji8ueAfwb+FKr9BRlTR7mh1z5f\n0ZzQRwD7ApZLODVxtpbxf/irgKxeiQ7wN/GcA7zdzuaPi8j7IvK8iEzupZAUeFFENojIona2B3NM\nw2khHX/JInG8WgxV1QP++YPA0HbKRPLYfQn3y6o9nb3n4bLY3xz0cAdNCJE8Xp8ADqnqjg62h/2Y\ntckNvfb5iuaEHtVEZADw/4B/VdXqNpvfxTUrTAP+B/hLL4V1gaqeC8wHvioiF/bSfjslIonAVcAz\n7WyO1PE6hbrfv1HT9UtEvgc0AX/soEgk3vPfAmcCHwMO4Jo3osl1nL52HtZjdrrcEO7PVzQn9FJg\nZMByrn9du2VEJB4YBFSEOzARScC9YX9U1f9ru11Vq1X1mH9+FZAgItnhjktVS/3TMuDPuJ+9gYI5\npuEyH3hXVQ+13RCp4xXgUEvTk39a1k6ZXj92InITcCVwvT8RnCKI9zzkVPWQqjarqg/43w72GZHP\nmj8PfBp4qqMy4TxmHeSGXvt8RXNCXw+MFZF8f+1uIbCiTZkVQMvZ4M8Ar3T0wQ8Vf/vc74Gtqvqr\nDsoMa2nLF5EZuOMc1n80IpImIukt87iTapvaFFsBfFGcWUBVwE/BcOuw1hSJ49VG4OfoRuCv7ZRZ\nDcwVkcH+Joa5/nVhISLzgO8AV6lqbQdlgnnPwxFb4HmXT3Wwz2C+v+FwGVCkqiXtbQznMTtNbui9\nz1eoz/SG+KzxAtyZ4p3A9/zr7sZ9yAGScT/hi4F3gDG9ENMFuJ9MHwAb/Y8FwFeAr/jLLAY2487s\nvwWc3wtxjfHv733/vluOV2BcAiz1H88PgYJeeh/TcAl6UMC6iBwv3D+VA0Ajrp3yy7jzLi8DO4A1\nQKa/bAHwUMBzv+T/rBUDN4c5pmJcm2rLZ6ylN9cZwKrTvee9cLz+4P/8fIBLVsPbxuZfPuX7G864\n/OsfbflcBZTtlWN2mtzQa58vGylqjDF9RDQ3uRhjjOkCS+jGGNNHWEI3xpg+whK6Mcb0EZbQjTGm\nj7CEbowxfYQldGOM6SMsoRtjTB/x/wHbdRt6c88szgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]}]}